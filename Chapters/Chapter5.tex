\chapter{Orthogonality}
\section{Linear Mapping}
\begin{definition}
Let us consider two vector spaces $V$ and $W$. A function (or mapping) $L:V\to W$ is called a \textit{linear mapping} if the following two conditions are satisfied:
\begin{enumerate}
\item For any $\vv\in V$ and $\vv'\in V$, $L(\vv+\vv') = L(\vv)+L(\vv')$
\item For any $\vv\in V$ and any scalar $\alpha$, $L(\alpha\vv) = \alpha\cdot L(\vv)$
\end{enumerate}
\end{definition}
\begin{example}
Let us consider matrix $A\in\R^{n,m}$. We define a linear mapping $L_A$ as follows:
\[
L_A(\vv) = A\vv\hspace{7mm} L_A:\R^m\to\R^n
\]	
Is $L_A$ a linear mapping? Yes. A linear mapping or transformation can always be represented as a matrix-vector product and vice-versa.
\begin{proof}
\begin{enumerate}
\item $\forall \vv,\vv'\in\R^m$, we have: $$L_A(\vv+\vv') = A(\vv+\vv') = A\vv+A\vv' = L_A(\vv) + L_A(\vv')$$
\item $\forall\vv\in\R^m,\forall\alpha$ ($\alpha$ is scalar), we have: $$L_A(\alpha\vv) = A(\alpha\vv) = \alpha\cdot A\vv = \alpha L_A(\vv)$$
\end{enumerate}
\end{proof}
\end{example}
\section{The 4 subspaces of a matrix}
Let us consider matrix $A\in\R^{n,m}$, $A:\R^m\to\R^n$. Let us consider the vector $\vv\in\R^m$.
\[
A\vv = \underbrace{v_1\cdot\colvec{3}{a_{11}}{\vdots}{a_{n1}} + v_2\cdot\colvec{3}{a_{12}}{\vdots}{a_{n2}}+ ... + v_m\cdot\colvec{3}{a_{1m}}{\vdots}{a_{nm}}}_{\text{Linear combination of columns of $A$}}
\]

\begin{center}
\begin{tikzpicture}[scale=1.2]

\draw[] (0,0) --(2,0) --(2,3) -- (0,3)--cycle;
\draw (2,3) node[anchor=north east]{$\R^m$};

\draw[] (5,0) --(7,0) --(7,3) -- (5,3)--cycle;
\draw (7,3) node[anchor=north east]{$\R^n$};

\draw[fill=black] (1,2) circle (0.03);
\draw[fill=black] (0.8,0.8) circle (0.03);
\draw (0.9,1.5) node[anchor=east]{0};
\draw[fill=black] (0.9,1.5) circle (0.03);
\draw (1,2) node[anchor=south]{$\vv$};
\draw[fill=black] (1.7,1.5) circle (0.03);

\draw[rotate=315,shift={(-2.75,3.75)}] (5.5,1) --(6.5,1) --(6.5,2.2) -- (5.5,2.2)--cycle;
\draw[->](5.9,1.4)--(6.3,1.4);
\draw[->](5.9,1.4)--(6.15,1.7);

\draw [->] (1,2) parabola bend (3.6,2.5) (6.2,2);
\draw [->] (0.8,0.8) parabola bend (3.4,0.6) (6,1);
\draw [->] (1.7,1.5) parabola bend (3.6,1.8) (5.7,1.5);
\draw (6.8,1.85) node[anchor=north east]{\tiny{$A\ul{v}$}};
\draw[<-] (6.2,1) -- (6.2,-0.5) node [anchor=north,xshift=-30]{span of columns of $A$};
\end{tikzpicture}
\end{center}

\begin{example}
\begin{enumerate}
\item 
\[
A\in\R^{2,2} = \begin{pmatrix}
1 & 3\\
1 & -1
\end{pmatrix}
\]
\begin{center}
\begin{tikzpicture}[scale=0.45]
\draw[->](-4,0)--(4,0) node[anchor=west]{$x_1$};
\draw[->](0,-4)--(0,4)node[anchor=south]{$x_2$};
\draw[fill=black] (1,2) circle (0.04);
\draw[fill=black] (1,-1) circle (0.04);

\newcommand{\offsetDrawing}{12}
\draw [->] (1,2) parabola bend (7,3) (1+\offsetDrawing,2);
\draw [->] (1,-1) parabola bend (7,-2) (1+\offsetDrawing,-1);



\draw[->](-4+\offsetDrawing,0)--(4+\offsetDrawing,0) node[anchor=west]{$x_1$};
\draw[->](0+\offsetDrawing,-4)--(0+\offsetDrawing,4)node[anchor=south]{$x_2$};

\draw[->,thick](0+\offsetDrawing,0)--(1+\offsetDrawing,1) node[anchor=west]{Col. 1 of $A$};
\draw[->,thick](0+\offsetDrawing,0)--(3+\offsetDrawing,-1) node[anchor=west]{Col. 2 of $A$};

\draw[] (-4,4) node{$\R^2$};
\draw[] (4+\offsetDrawing,4) node{$\R^2$};
\end{tikzpicture}
\end{center}
\item \[
A = \begin{pmatrix}
1 & 3\\
2 & 6
\end{pmatrix}
\]
\begin{center}
\begin{tikzpicture}[scale=0.45]
\draw[->](-4,0)--(4,0) node[anchor=west]{$x_1$};
\draw[->](0,-4)--(0,4)node[anchor=south]{$x_2$};
\draw[fill=black] (0.8,1.2) circle (0.04);
\draw[fill=black] (1,-1) circle (0.04);
\draw[fill=black] (-1,2) circle (0.04);
\newcommand{\offsetDrawing}{12}
\draw [->] (0.8,1.2) parabola bend (5.5,2) (-0.25+\offsetDrawing,-0.5);
\draw [->] (1,-1) parabola bend (6.25,-1.5) (-0.5+\offsetDrawing,-1);
\draw [->] (-1,2) parabola bend (5,3) (0.95+\offsetDrawing,2);



\draw[->](-4+\offsetDrawing,0)--(4+\offsetDrawing,0) node[anchor=west]{$x_1$};
\draw[->](0+\offsetDrawing,-4)--(0+\offsetDrawing,4)node[anchor=south]{$x_2$};

\draw[->,thick](0+\offsetDrawing,0)--(0.5+\offsetDrawing,1) ;
\draw[->,thick](0+\offsetDrawing,0)--(1.5+\offsetDrawing,3);
\draw[](-1.5+\offsetDrawing,-3)--(1.75+\offsetDrawing,3.5);

\draw[] (-4,4) node{$\R^2$};
\draw[] (4+\offsetDrawing,4) node{$\R^2$};
\draw[] (3.4+\offsetDrawing,3.2) node{$\leftarrow$ span of cols. of $A$};
\end{tikzpicture}
\end{center}

\end{enumerate}
	
\end{example}

\begin{note}
In order for a solution of $A\ul{x} = \ul{b}$ to exist, $\ul{b}$ should belong to a \textit{span of the columns} of the matrix $A$.	
\end{note}

\begin{definition}
The span (or all possibile linear combinations) of columns of the matrix $A \in \R^{n, m}$ is called a \textit{column space} of $A$, denoted by $C(A)$, where $C(A)\subset R^n$.
\end{definition}
\begin{definition}
Let us consider the matrix $A\in\R^{n,m}, A:\R^m\to \R^n$. The null space of $A$ is defined as 
\[
N(A) = \left\{ \vv\in\R^m\mid A\vv = \ul{0}\right\}, N(A)\subset\R^m
\]
\begin{center}
\begin{tikzpicture}[scale=0.75]

\draw[] (0,0) --(3,0) --(3,5) -- (0,5)--cycle;
\draw (3,5) node[anchor=north east]{$\R^m$};

\draw[] (7,0) --(10,0) --(10,5) -- (7,5)--cycle;
\draw (10,5) node[anchor=north east]{$\R^n$};

\draw[rotate=-45,xshift=-55] (0.5,2) --(0.5,4) --(2,4) -- (2,2)--cycle;
\draw[->](1.65,2.65) -- (1.65,3.10);
\draw[->](1.65,2.65) -- (2.10,2.65);

\draw(0.5,2.5) node[anchor=west]{\tiny{$N(A)$}};
\draw(2.75,2.9) node[anchor=east]{\tiny{$\vv$}};
\draw[fill=black](2.55,2.7) circle (0.04);
\draw[fill=black](1.5,2) circle (0.04);

\draw[->](8.5,2.5) -- (8.5,3.5);
\draw[->](8.5,2.5) -- (9.5,2.5);

\draw [->] (2.55,2.7) parabola bend (5.5,3.3) (8.5,2.5);
\draw [->] (1.5,2) parabola bend (5,1.5) (8.5,2.5);

\end{tikzpicture}
\end{center}

\end{definition}
\begin{example}
Consider the following matrix
\[
A = \begin{pmatrix}
1 & 3 \\
2 & 6
\end{pmatrix}
\]	
\textit{What is the nullspace of $A$?} \\ \\
We should find all solutions for the following equation 
$$A\ul{x} = \ul{0}$$
\[
\begin{rightalignedcases}
x_1 + 3x_2 = 0\\
2x_1+6x_2 = 0
\end{rightalignedcases} \to\begin{rightalignedcases}
x_1+3x_2=0\\
0 = 0
\end{rightalignedcases}\to\begin{rightalignedcases}
x_1=-3x_2\\
0 = 0
\end{rightalignedcases}\] \\
The nullspace of this matrix will be a line formed by a linear combination of the vector $\colvec{2}{-3}{1}$, or in other words $\alpha\cdot\colvec{2}{-3}{1}$, for all possible $\alpha$, or in other words it will be the $span(\colvec{2}{-3}{1})$.
\[
x_1 = -3x_2 = -3\alpha,x_2=\alpha\to\alpha\colvec{2}{-3}{1},\alpha\colvec{2}{-6}{2}
\]

\begin{center}
\begin{tikzpicture}[scale=0.45]
\draw[->](-4,0)--(4,0) node[anchor=west]{$x_1$};
\draw[->](0,-4)--(0,4)node[anchor=south]{$x_2$};

\newcommand{\offsetDrawing}{12}
\draw [->] (-1.7,1.7) parabola bend (5.15,3) (-0.05+\offsetDrawing,0.05);
\draw [->] (1.2,-1.2) parabola bend (5.9,-2) (-0.05+\offsetDrawing,-0.05);
\draw[fill=black] (0+\offsetDrawing,0) circle (0.05);
\draw[->](-4+\offsetDrawing,0)--(4+\offsetDrawing,0) node[anchor=west]{$x_1$};
\draw[->](0+\offsetDrawing,-4)--(0+\offsetDrawing,4)node[anchor=south]{$x_2$};
\draw(-3,3) -- (3,-3);
\draw[thick,->](0,0) -- (1,-1);

\draw[] (-4,4) node{$\R^2$};
\draw[] (4+\offsetDrawing,4) node{$\R^2$};
\draw[] (3,-3) node[anchor=west]{$\leftarrow N(A)$};
\end{tikzpicture}
\end{center}
\end{example}

\begin{theorem}
The nullspace, $N(A)$, of $A\in\R^{n,m}$ is a subspace of $\R^m$.
\end{theorem}
\begin{proof}
Let us assume that $\ul{x},\ul{x}'\in N(A)$ and $\alpha$ is arbitrary scalar. 
\begin{enumerate}
\item $A(\ul{x}+\ul{x}') = A\ul{x}+A\ul{x}' = \ul{0} + \ul{0} = \ul{0} \Rightarrow (\ul{x}+\ul{x}')\in N(A)$\\
\item $A(\alpha\ul{x}) = \alpha (A\ul{x}) = \alpha\cdot \ul{0} = \ul{0}\Rightarrow \alpha\ul{x}\in N(A)$
\end{enumerate}
\end{proof}

\begin{theorem}
The column space, $C(A)$, of $A\in\R^{n,m}$ is a subspace of $\R^n$.
\end{theorem}

\begin{definition}
The row space of a matrix $A \in R^{n, m}$ is the span of the rows of $A$. Clearly, $R(A)=C(A^T)$ and $R(A)\subset \R^m$.
\end{definition}
\begin{definition}
The left nullspace of $A$ is defined as $N(A^T)$. $N(A^T)\subset\R^n$.
\end{definition}
\begin{theorem}
$R(A)$ is a subspace of $\R^m$.
\end{theorem}
\begin{proof}
Same as for the proof that $C(A)$ is a subspace of $\R^{n}$, but for $A^T$.
\end{proof}
\begin{theorem}
$N(A^T)$ is a subspace of $\R^n$.
\end{theorem}
\begin{proof}
Same as for $N(A)$ but replace $A$ with $A^T$.
\end{proof}

\begin{theorem}
$R(A)$ and $N(A)$ are \textit{orthogonal subspaces} in $\R^m$ for $A\in\R^{n,m}$.
\end{theorem}

\begin{proof}
Let us consider $\forall \ul{x}\in N(A), A\ul{x} = \ul{0}$
\[
A\ul{x} = \colvec{3}{-\text{ row 1 of }A\to}{\vdots}{-\text{ row $n$ of }A\to}\cdot \colvec{3}{\mid}{x}{\downarrow} = \colvec{3}{<\text{ row 1 of }A,\ul{x}>}{\vdots}{{<\text{ row $n$ of }A,\ul{x}>}} \mathop=\limits^{\ul{x}\in N(A)}\colvec{3}{0}{\vdots}{0}
\]
$\ul{x}$ is orthogonal to every row of $A$. $\ul{x}$ is orthogonal to every linear combination of rows of $A$. $\ul{x}$ is orthogonal to $R(A)$. In fact, what we just showed is that $N(A)$ and $R(A)$ are \textit{orthogonal complements}. 
\end{proof}

\begin{theorem}
$N(A^T)$ and $C(A) = R(A^T)$ are orthogonal complements in $\R^n$
\end{theorem} \\ \\
Suppose we have the following matrix $A\in\R^{n,m}:\R^m\to\R^n$. \\  
The row rank of $A = \rank(A) = \dim\left( R(A) \right) =  \dim\left( C(A) \right) = $ column rank of $A$.
 
\begin{align*}
N(A) &:A\ul{x} = \ul{0}\hspace{5mm}\forall x\in\R^m\\ \\
C(A) &: A\ul{v} = \text{ Linear combinations of columns of $A$}\\
&\hspace{7.5mm}= v_1\cdot\text{col 1 of $A$}+\dots+v_n\cdot\text{col $n$ of $A$}\in \R^n
\end{align*}

\begin{theorem}
$N(A)$ is an orthogonal complement of $R(A)$ in $\R^m$,
\[
\dim N(A)+\underbrace{\dim R(A)}_{=\rank(A)}=m
\]
\end{theorem}
\begin{theorem}
$N(A^T)$ is an orthogonal complement of $R(A^T) = C(A)$ in $\R^n$,
\[
\dim N(A^T)+\underbrace{\dim C(A)}_{=\rank(A)}=n
\]
\end{theorem}
Let us consider $A\in\R^{n,m},A:\R^m\to\R^n$ and $\rank(A)=r$
\begin{center}
{\begin{tikzpicture}[scale=0.6, every node/.style={transform shape}]
\path[use as bounding box] (-2,-8) rectangle (17,4);
\draw[rotate=-45](0,0)--(0,5)--(3,5)--(3,0)--cycle;
\draw[rotate=45,shift={(-3,-8)}](0,0)--(0,5)--(3,5)--(3,0)--cycle;
\draw[rotate=-45,shift={(3,0)}] (0.3,0) -- (0.3,0.3) -- (0,0.3);
\draw[rotate=-45,shift={(2.25,-0.75)},dashed] (2,0) -- (2,2) -- (0,2);
\draw[fill=black](3.85,-2.15) circle (0.075);
\draw[fill=black](3.85,-2.15) node[anchor=west]{$x_r+x_n$};

\draw (0,-2.15) node {2 orthogonal spaces};
\draw[->](0,-2)--(0.75,-1.25);
\draw[->](0,-2.4)--(0.75,-3.2);
\draw[->](-0,15)--(20,15);
\draw (3.5,-7) node {$N(A)$};
\draw (3.5,2.8) node {$R(A)$};
\draw (3,1.5) node {$\dim R(A) = r$};
\draw (3,-5.5) node {$\dim N(A) = m-r$};
\draw (2.5,-3.8) node {$\ul{x}_n$};
\draw (2.5,-0.5) node {$\ul{x}_r$};
\draw (0,3) node {$\R^n$};

\begin{scope}[xshift=300]
\draw[rotate=-45](0,0)--(0,5)--(3,5)--(3,0)--cycle;
\draw[rotate=45,shift={(-3,-8)}](0,0)--(0,5)--(3,5)--(3,0)--cycle;
\draw[rotate=-45,shift={(3,0)}] (0.3,0) -- (0.3,0.3) -- (0,0.3);
\draw (3.5,-7) node {$N(A^T)$};
\draw (3.5,2.8) node {$C(A)$};
\draw (3,1.5) node {$\dim C(A) = r$};
\draw (3.5,-5.5) node {$\dim N(A^T) = n-r$};
\draw (2.5,-0.5) node {$A\ul{x}_r = \ul{b}$};
\draw (0,3) node {$\R^n$};
\end{scope}

\draw[->](2.75,-0.5) --(12,-0.5);
\draw[->] (5.2,-2.15) -- (12,-0.7);
\draw[] (7.8,-0.9) node {$A(x_r+x_n)=Ax_r+Ax_n = Ax_r = b$};
\draw[->] (2.75,-3.8) -- (12.3,-2.15);
\end{tikzpicture}}
\end{center}

\begin{lemma}
For any vector $\ul{b}$ in $C(A)$, there exists \textit{one and only one} vector $\ul{x}_r\in R(A)$ such that $$A\ul{x}_r=b$$
\end{lemma}

\begin{proof}
Let us assume that $\ul{x}_r$ and $\ul{x}_r'$ are in the row space, $R(A)$. Let us assume that $A\ul{x}_r = A\ul{x}_r'$. We have
\[
\ul{x}_r - \ul{x}_r' \in R(A)
\]
But we also have 
\[
A\ul{x}_r -A\ul{x}_r' = \underbrace{A(\ul{x}_r-\ul{x}_r')}_{\in N(A)} = \ul{0} 
\]
It means that $(\ul{x}_r-\ul{x}_r' )$ is in $R(A)$ and $N(A)$, but they are orthogonal subspaces, therefore 
\[
\ul{x}_r-\ul{x}_r' =\ul{0}\Rightarrow \ul{x}_r=\ul{x}_r' 
\]
\end{proof}
\begin{example}
Let us consider
\[
A = \begin{pmatrix}
1 & -2 \\
-1 & 2
\end{pmatrix}\in\R^{2,2}
\]	
\begin{center}
\resizebox{\linewidth}{!}{
\begin{tikzpicture}[scale=0.8]
\newcommand{\offset}{0.1}

\foreach \x in {-6,...,6}{
	\draw (\x,\offset) -- (\x,-\offset);
	\draw (\offset,\x) -- (-\offset,\x);
	
}
\foreach \x in {-5,5,5}{
	\ifthenelse{\NOT\x=0}{
		\draw (\x,0) node[anchor=north]{$\x$};
		\draw (-\offset,\x) node[anchor=east]{$\x$};
	}{};
}


\draw[->] (-6.5,0) -- (6.5,0) node[anchor=west]{$x_1$};
\draw[->] (0,-6.5) -- (0,6.5)node[anchor=south]{$x_2$};

\draw[thick](3,-6) -- (-3,6);
\draw(3,-6) node[anchor=west]{$R(A)$};
\draw[thick,->](0,0) -- (1,-2);

\draw[very thick](-6,-3) -- (6,3);
\draw(6,3) node[anchor=west]{$N(A)$};
\draw[very thick,->](0,0) -- (2,1);

\draw(-7,-1) -- (5,5);

\begin{scope}[xshift=483]
\foreach \x in {-6,...,6}{
	\draw (\x,\offset) -- (\x,-\offset);
	\draw (\offset,\x) -- (-\offset,\x);
	
}
\foreach \x in {-5,5,5}{
	\ifthenelse{\NOT\x=0}{
		\draw (\x,0) node[anchor=north]{$\x$};
		\draw (-\offset,\x) node[anchor=east]{$\x$};
	}{};
}


\draw[->] (-6.5,0) -- (6.5,0) node[anchor=west]{$x_1$};
\draw[->] (0,-6.5) -- (0,6.5)node[anchor=south]{$x_2$};

\draw[very thick](-6,-6) -- (5,5) node [anchor=west]{$N(A^T)$};
\draw[thick,->](0,0) -- (1,-1);
\draw[thick](-6,6) -- (5,-5) node [anchor=west]{$C(A)$};

\draw[](6,6)node[]{$\R^2(\R^n)$};
\end{scope}

\draw[fill=black] (12,5) circle (0.1) node[anchor=west,xshift=15]{$A\ul{x}_r=\colvec{2}{-5}{5}$};
\draw[fill=black] (1,3) circle (0.05) node[anchor=west,xshift=15]{$\ul{x}_r+\ul{x}_n=\colvec{2}{1}{3}$};
\draw[fill=black] (2,1) circle (0.05)node[anchor=north,yshift=-5,xshift=5]{$\ul{x}_n$};
\draw[fill=black] (-1,2) circle (0.05)node[anchor=north,xshift=-2]{$\ul{x}_r$};

\draw[dashed] (1,3) -- (2,1);

\draw [->] (1,3) parabola bend (6.5,6) (12,5);
\draw [->,very thick] (2,1) parabola bend (9.5,2) (16.9,0.15);
\draw [->,thick] (-1,2) parabola bend (5.5,7) (12,5);
\draw[](-6,6)node[]{$\R^2(\R^m)$};
\end{tikzpicture}}
\end{center}
\end{example}
\textit{What is the row space of $A$?} \\ \\
It is basically all possible linear combinations of the row vectors of $A$:
\[
R(A) = \text{span}\left\{ \colvec{2}{1}{-2},\colvec{2}{-1}{2}\right\} = \text{span} \left\{ \colvec{2}{1}{-2}\right\}
\]
Note that the vectors are linearly dependent. Observe $\rank(A) = 1$, then $\dim(R(A))=1$. \\ \\
\textit{What is the nullspace of $A$?} \\ \\
We need basically to find the solutions to the following equation $$A\ul{x} = \ul{0}$$
\[
\Rightarrow \begin{rightalignedcases}
x_1-2x_2 = 0\\
-x_1+2x_2 = 0\\
\end{rightalignedcases} \Rightarrow\begin{rightalignedcases}
x_1-2x_2 = 0\\
0 = 0\\
\end{rightalignedcases}\Rightarrow x_1 = 2x_2\text{ (which represents a line)}
\]\\
The dimention of $N(A)$ is equal to the number of columns minus the dimension of the row space of $A$, so $\dim(N(A)) = 2 - 1 = 1$. \\ \\
\textit{What is the column space of $A$?} \\ \\
The dimension of $C(A)$ is equal to the dimension of $R(A)$. The column space is defined as a linear combination of the column vectors of $A$:
\[
C(A) = \text{span}\left\{ \colvec{2}{1}{-1},\colvec{2}{-2}{2}\right\} = \text{span}\left\{\colvec{2}{1}{-1}\right\}
\]
\textit{What is the left null space of $A$?} \\ \\
First, $\dim N(A^T) = 2-1=1$. \\ \\
Now, consider 
\begin{align*}
\ul{x}_r &= \colvec{2}{-1}{2}\Rightarrow A\ul{x}_r =\begin{pmatrix}
1 & -2\\
-1 & 2	
\end{pmatrix}\colvec{2}{-1}{2} = \colvec{2}{-5}{5}\\
\ul{x}_n &= \colvec{2}{2}{1}\Rightarrow A\ul{x}_n =\begin{pmatrix}
1 & -2\\
-1 & 2	
\end{pmatrix}\colvec{2}{2}{1} = \colvec{2}{0}{0}
\end{align*}

\section{Orthogonal Basis and Gram-Schmidt process}
\subsection{Orthogonal and Orthonormal}
\begin{definition}
Vectors $\ul{q}_1,\dots,\ul{q}_m$ are \textit{orthogonal} if:
\[
\langle \ul{q}_i,\ul{q}_j\rangle = \ul{q}_i^T\ul{q}_j=0\hspace{5mm}\text{if }i\not=j
\]
In other words, two vectors $\ul{q}_i$ and $\ul{q}_j$ are orthogonal if their dot product is zero, so two orthogonal vectors must be perpendicular.
\end{definition}

\begin{definition}
Vectors $\ul{q}_1,\dots,\ul{q}_m$ are \textit{orthonormal} if:
\[
\langle \ul{q}_i,\ul{q}_j\rangle = \ul{q}_i^T\ul{q}_j=\begin{rightalignedcases}
0 & \text{if }i\not=j\\
1 & \text{if }i=j\\
\end{rightalignedcases}
\]
In other words, two vectors $\ul{q}_i$ and $\ul{q}_j$ are orthonormal if they are orthogonal and they both have length of $1$ (they are \textit{unit vectors}).
\end{definition}
If the columns of the matrix are orthonormal vectors, then this matrix is usually denoted by $Q$. In this case, we have $$Q^TQ=I$$ Note that if $Q$ is \textit{not} a square matrix, then $QQ^T$ is not necessarily $I$.

\begin{definition}
A square matrix is called orthogonal (if its columns are orthonormal vectors) if $Q^TQ=I$. In this case, since it is a square matrix, $QQ^T=I$
\end{definition}

\subsection{Projection onto a Line}
Let us assume that we have a line \textcolor{red}{$L$} which is the span of the vector $\ul{a}=\colvec{3}{a_1}{\vdots}{a_n}\in\R^n$, and suppose we have another vector $\ul{b}\in\R^n$. We want to find the vector \textcolor{blue}{$\ul{p}$} belonging to the line $L$ (spanned by $\ul{a}$), closest to vector $\ul{b}$. In other words, we are looking for the vector $\ul{p}$, which is the orthogonal projection of $\ul{b}$ onto the line given by $\ul{a}$.

% THIS PICTURE IS NOT ACCURATE AND CAN BE IMPROVED...
\begin{center}
\begin{tikzpicture}[scale=0.75]

% x axis
\draw[->] (-1,0) -- (5,0);
% y axis
\draw[->] (0,-1) -- (0,5);

% line L
\draw[red] (-0.75,-1.5) -- (2.65, 5.3);

% vector a
\draw[->,thick] (0,0) -- (2,4);

% projection p
\draw[blue, ->,thick] (0,0) -- (1.5, 3);

% vector b
\draw[->,thick] (0,0) -- (2.95, 2.17);

% vector e
\draw[green, ->] (1.5,3) -- (2.94, 2.13);

% letter a
\draw (1.6, 4) node {$\ul{a}$};

% letter L
\draw (2.6, 4.5) node {$L$};

% letter p
\draw (1.1, 2.9) node {$\ul{p}$};

% letter b
\draw (3.1, 2.1) node {$\ul{b}$};

% letter e
\draw (2.2,2.9) node {$\ul{e}$};

\draw[yshift=73,xshift=35,scale=0.1] (2.1,3) -- (3.5,2.17);
\draw[yshift=73,xshift=53.5,scale=0.1,rotate=90] (2.1,3) -- (3.5,2.17);
\end{tikzpicture}

\end{center}
$\ul{p}$ is vector multiple of the vector $\ul{a}$ (it has the same direction, but possibly not the same length) $$\ul{p} = c\ul{a}$$ where $c$ is some scalar. \\ \\
Now, let us define the so-called \textit{error vector} \textcolor{green}{$\ul{e}$} in the following way $$\ul{e} = \ul{b}-\ul{p} = \ul{b} -c\ul{a}$$ As you can see from the picture, the error vector $\ul{e}$ is orthogonal to the line, therefore 
\begin{align*}
\langle\ul{a},\ul{e} \rangle &= 0\\
\langle\ul{a},\ul{e} \rangle &=\ul{a}^T \cdot (\ul{b}-c\ul{a}) = \ul{a}^T\cdot  \ul{b} - c(\ul{a}^T\cdot \ul{a}) = 0\\
\Rightarrow c &= \frac{\ul{a}^T\ul{b}}{\ul{a}^T\ul{a}}\\
\end{align*}
Note that $c$ is the scalar that you multiply by $\ul{a}$ to obtain $\ul{p}$, therefore
\begin{align*}
\ul{p} &= c\ul{a} = \ul{a}c = \ul{a}\frac{\ul{a}^T\ul{b}}{\ul{a}^T\ul{a}} = \underbrace{\frac{\ul{a}\ul{a}^T}{\ul{a}^T\ul{a}}}_{P\in\R^{n,n}\text{ (projection matrix)}}\cdot\ul{b}
\end{align*}
\begin{example}
Let us consider $\ul{a}=\colvec{3}{1}{2}{2} \in\R^3$. We first want to find the projection matrix $P$:
\[
P = \frac{\ul{a}\cdot \ul{a^T}}{\ul{a}^T\ul{a}} = \colvec{3}{1}{2}{2}\cdot \begin{pmatrix}
1 & 2 & 2
\end{pmatrix}\cdot\frac{1}{9} = \frac{1}{9}\cdot\begin{pmatrix}
1 & 2 & 2\\
2 & 4 & 4\\
2 & 4 & 4
\end{pmatrix}
\]
Note that the numerator produces a matrix, but the denominator produces a number which is equivalent to the dot product of $a$ with itself.\\
Let us project the following vector $\ul{b} = \colvec{3}{1}{1}{1}$ onto the line spanned by $\ul{a}$. We just need to multiply $P$ by $\ul{b}$:
\[\ul{p}=P\ul{b} = \frac{1}{9}\begin{pmatrix}
1 & 2 & 2\\
2 & 4 & 4\\
2 & 4 & 4
\end{pmatrix}\colvec{3}{1}{1}{1} = \frac{1}{9}\colvec{3}{5}{10}{10}
\]
\end{example}
\begin{note}
$\ul{p}^2 = \ul{p}$
\end{note}
\begin{note}
$(I - P)-$ projection onto subspace orthogonal to the line given by $\ul{a}$
\end{note}

\subsection{Gram-Schmidt process}
Given linearly independent vectors $\ul{a},\ul{b},\ul{c},\dots$ (note that linearly indepence does not necessarily mean that the vectors are perpendicular to each other!), the Gram-Schmidt process allows us to construct an \textit{orthogonal basis} of span ${\ul{a},\ul{b},\ul{c},\dots}\in\R^n$: we first want to find orthogonal vectors $\ul{a}',\ul{b}',\ul{c}',\dots$ which span the same subspace as $\ul{a},\ul{b},\ul{c},\dots$, and then we normalise them (we make them unit vectors by dividing each of them by their respective length).\\ \\
The general process is the following:
\begin{enumerate}
\item Choose $\ul{a}' = \ul{a}$
\item It is likely that $\ul{b}$ is not orthogonal to $\ul{a}'$, so we need to subtract the projection of $\ul{b}$ onto $\ul{a}'$ from $\ul{b}$:
\[
\ul{b}' = \ul{b}-\frac{\ul{a'}^T\ul{b}}{\ul{a'}^T\ul{a'}}\ul{a}'
\]
Note that $\ul{a} = \ul{a'}$, so it does not matter if we calculate the projection of $\ul{b}$ onto $\ul{a'}$ or $\ul{a}$ (in this case). If you are still not familiar how projections work, try to draw the situation on a paper, and you will definitely understand better.
\item $\ul{c}'$ is likely not orthogonal to $\ul{a}'$ and $\ul{b}'$. Again, subtract its projections
\[
\ul{c}' = \ul{c}-\frac{\ul{a'}^T\ul{c}}{\ul{a'}^T\ul{a'}}\ul{a}'-\frac{\ul{b'}^T\ul{c}}{\ul{b'}^T\ul{b'}}\ul{b}'
\]
and so on.
\item Finally, normalise $\ul{a'}$, $\ul{b'}$, $\ul{c'}$, ... 
\[
\ul{\hat{q}}_1 = \frac{\ul{a}'}{\norm{\ul{a}'}}, \hspace{3mm} \ul{\hat{q}}_2 = \frac{\ul{b}'}{\norm{\ul{b}'}},\hspace{3mm} \ul{\hat{q}}_3 = \frac{\ul{c}'}{\norm{\ul{c}'}},\dots
\]
Note that $\ul{\hat{q}}_1$, $\ul{\hat{q}}_2$, $\ul{\hat{q}}_3$, ... are the orthogonal unit vectors, which form a orthogonal basis. Vectors that are denoted with \^{} over them are often intended to be \textit{unit vectors}.
\end{enumerate}
\begin{example}
Suppose we have the following linearly independent vectors
\[
\ul{a} = \colvec{3}{1}{-1}{0},\ul{b} = \colvec{3}{2}{0}{-2},\ul{c} = \colvec{3}{3}{-3}{3}
\]	
We first need to find $\ul{a}',\ul{b}',\ul{c}'$, and then the unit vectors $\ul{\hat{q}}_1,\ul{\hat{q}}_2,\ul{\hat{q}}_3$. 
\begin{enumerate}
\item \[
\ul{a}' = \ul{a} = \colvec{3}{1}{-1}{0}
\]
\item \[
\ul{b'} = \ul{b} -\frac{\ul{a'}^T\ul{b}}{\ul{a'}^T\ul{a'}}\ul{a}' = \colvec{3}{2}{0}{-2} - \frac{\left\langle \colvec{3}{1}{-1}{0}, \colvec{3}{2}{0}{-2}\right\rangle}{\colvec{3}{1}{-1}{0} \cdot \colvec{3}{1}{-1}{0}} = \colvec{3}{1}{1}{-2}
\]
\item \[
\ul{c}' = \ul{c}-\frac{\ul{a'}^T\ul{c}}{\ul{a'}^T\ul{a'}}\ul{a}'-\frac{\ul{b'}^T\ul{c}}{\ul{b'}^T\ul{b'}}\ul{b}' = \colvec{3}{1}{1}{1}
\]
We can try to check if $\ul{a'}$, $\ul{b'}$ and $\ul{c'}$ are perpendicular to each other by checking if their dot product is equals to zero:
$$\langle \ul{a}',\ul{b}'\rangle = 0$$
$$\langle \ul{a}',\ul{c}'\rangle = 0$$
$$\langle \ul{b}',\ul{c}'\rangle = 0$$
\item Finally, we normalise $\ul{a'}$, $\ul{b'}$ and $\ul{c'}$:

$$\ul{\hat{q}}_1 = \frac{\ul{a}'}{\norm{\ul{a}'}} = \frac{1}{\sqrt{2}}\colvec{3}{1}{-1}{0}$$
$$\ul{\hat{q}}_2 = \frac{\ul{b}'}{\norm{\ul{b}'}} = \frac{1}{\sqrt{6}}\colvec{3}{1}{1}{-2}$$
$$\ul{\hat{q}}_3 = \frac{\ul{c}'}{\norm{\ul{c}'}} = \frac{1}{\sqrt{3}}\colvec{3}{1}{1}{1}$$
\end{enumerate}
\end{example}

% THIS SECTION THIS TO BE IMPROVED, IN MY OPINION IT IS NOT UNDERSTANDABLE!!!
\subsection{Projection onto a Subspace}
Assume we have linearly independent vectors $a_1,\dots,a_m\in\R^n$. We want to project the vector $\ul{b}\in\R^n$ onto the subspace $V$ spanned by $a_1,\dots,a_m$. Subspace consists of all linear combinations
\[
x_1a_1+\dots+x_ma_m =\underbrace{\begin{pmatrix}
\mid & {} & \mid\\
a_1 & \dots & a_m\\
\downarrow & {} & \downarrow 
\end{pmatrix}}_{A\in\R^{n,m}}\cdot \underbrace{\hat{x}}_{\in\R^m}
\]
We are looking for the projection $\ul{p}$ of $\ul{b}$ onto subspace $V$. We can define $\ul{e}=\ul{b}-\ul{p}$, where $\ul{e}$ should be orthogonal to all $a_1,\dots,a_m$
\[
\begin{rcases*}
\langle a_1,\ul{e}\rangle = \ul{a_1}^T\cdot (\ul{b}-A\hat{x}) = 0\\
\hspace{20mm}\vdots \\
\langle a_m,\ul{e}\rangle = \ul{a_m}^T\cdot (\ul{b}-A\hat{x}) = 0
\end{rcases*}\Rightarrow \underbrace{\colvec{3}{-\ul{a}_1^T\to}{\vdots}{-\ul{a}_m^T\to}}_{A^T}(\ul{b}-A\hat{x})=0
\]
\begin{align*}
A^T(\ul{b}-A\hat{x}) &= 0\\
A^T\ul{b}-A^TA\hat{x} &= 0
\end{align*}
\begin{theorem}
$A$ has linearly independent columns. Then $A^TA$ is:
\begin{itemize}
\item Square
\item Symmetric
\item Invertible
\end{itemize}
\begin{align*}
\ul{\hat{x}} &= A(A^TA)^{-1}A^T\ul{b}\\
\ul{p} &= A\hat{x} = \underbrace{A(A^TA)^{-1}A^T}_{P\text{ - Proj. matrix}}\cdot\ul{b}\text{ - Projection vector}
\end{align*}

\end{theorem}

% Page 41 at the middle/top